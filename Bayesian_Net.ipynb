{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%run \"bn_utils.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_PATH = \".\\inputs\"\n",
    "OUT_PATH = \".\\outputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exact Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_inference(query, evidence, cpts, graph):\n",
    "    new_cpts = []\n",
    "    parents = graph['parents_nodes']\n",
    "    for i, cpt in enumerate(cpts):\n",
    "        tb = []\n",
    "        for row in cpt:\n",
    "            if evidence.get(i) and evidence[i] != row[i]:\n",
    "                continue\n",
    "            flag = True\n",
    "            for j in parents[i]:\n",
    "                if evidence.get(j) and row[j] != evidence[j]:\n",
    "                    flag = False\n",
    "            if flag:\n",
    "                tb.append(row)\n",
    "        new_cpts.append(tb)\n",
    "    return variable_elimination(evidence, query, new_cpts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model\n",
    "You should read the bayesian network from the *path* and output:\n",
    "1. conditional probability tables (CPTs) [list of list of dictionary]\n",
    "    - each member (cpt) is a list (the list at index *i* is cpt of vertex *i* )\n",
    "    - each member of cpt is a dictionary:\n",
    "        - the dictionary contains *|parents(v)| + 2* keys. (the value of parents of *v*, the value of *v* and *'prob'*)\n",
    "        - e.g:\n",
    "            *{\n",
    "                V<sub>1</sub>: True,\n",
    "                V<sub>2</sub>: False,\n",
    "                ...\n",
    "                V<sub>v</sub>: True,\n",
    "                'prob': 0.66\n",
    "            }*\n",
    "<br/><br/>\n",
    "2. graph (bayesian network) [dictionary of list of list]\n",
    "    - the keys are *'parents_nodes'* and *'children_nodes'*\n",
    "    - the value of each key is a list of list; the element at index *i* is the parents/children of vertex *i*\n",
    "<br/><br/>\n",
    "3. V (the number of variables/vertexes) [integer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Sampling\n",
    "\n",
    "You should implement Prior Sampling.\n",
    "\n",
    "1. First, sort the vertices topologically (We have done this for you; you just need to call the *topological_sort* function)\n",
    "\n",
    "2. Sample each vertex in topological order\n",
    "    - You can use np.random.random() function to generate a random number between 0 and 1\n",
    "        - To propose, the *sample_vertex* function which is defined in *bn_utils* file, can be useful\n",
    "<br/><br/>\n",
    "\n",
    "3. Take enough samples from the whole bayes net, say 10000\n",
    "\n",
    "4. Calculate the approximate probability of the query respecting to the evidence.\n",
    "    - Calculate # of samples that are consistent with the evidence\n",
    "    - Calculate # of samples that the query and the evidence occurred at the same time\n",
    "    - The conditional probability is obtained by dividing the first item by the second\n",
    "\n",
    ">Notice how this wouldn't work when we don't have samples that are consistent with evidence.\n",
    "<br/><br/>\n",
    "\n",
    "**Feel free to use each function in the *bn_utils* file whenever you need**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_sample(query, evidence, cpts, graph, n):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rejection Sampling\n",
    "\n",
    "This is almost identical to Prior Sampling except that we reject samples that are inconsistent with the evidence.\n",
    "\n",
    "1. First, sort the vertices topologically (Done for you! you just need to call the *topological_sort* function)\n",
    "\n",
    "2. Sample each vertex in topological order\n",
    "    - You can use np.random.random() function to generate a random number between 0 and 1\n",
    "        - To propose, the *sample_vertex* function which is defined in *bn_utils* file, can be useful\n",
    "    - Do not continue sampling the whole bayes net if you encounter a sampled vertex which is not consistent with the evidence; This can be more efficient in the context of time and resource.\n",
    "<br/><br/>\n",
    "3. Take enough samples from the whole bayes net, say 10000\n",
    "\n",
    "4. Calculate the approximate probability of the query respecting to the evidence.\n",
    "    - Calculate # of samples that the query and the evidence occurred at the same time\n",
    "    - The conditional probability is obtained by dividing the above value by the total unrejected samples (notice that here all samples are consistent with the evidence because inconsistent sample were rejected)\n",
    "\n",
    ">Notice how this still can be resource and time consuming when you go down into a deep bayesian net and reject a whole sample just because of one inconsistent vertex\n",
    "<br/><br/>\n",
    "\n",
    "**Feel free to use each function in the *bn_utils* file whenever you need**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejection_sample(query, evidence, cpts, graph, n):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood Sampling\n",
    "\n",
    "To overcome the problem of rejected samples, we can force the samples to be consistent with the evidence without even sampling the vertices that appear in the evidence.\n",
    "\n",
    "1. First, sort the vertices topologically (Done for you! you just need to call the *topological_sort* function)\n",
    "\n",
    "2. Sample each vertex in topological order\n",
    "    - If the vertex appears in the evidence, just append the probability that the variable has the same value as in the evidence, to a list of weights\n",
    "    - Otherwise, sample the vertex as usual\n",
    "    - On sampling the whole bayes net, you should calculate the weight of that sample by multiplying the weights in the list of weights\n",
    "    - Add the sample's weight to a list, say sample_weights\n",
    "<br/><br/>\n",
    "\n",
    "3. Take enough samples from the whole bayes net, say 10000\n",
    "\n",
    "4. Calculate the approximate probability of the query respecting to the evidence.\n",
    "    - Calculate sum of samples' weights that the query and the evidence occurred at the same time\n",
    "    - The conditional probability is obtained by dividing the above value by the sum of all samples' weights\n",
    "\n",
    ">Notice that in this approach, when sampling vertices, we do not consider the evidence variables that are deeper in the bayes net\n",
    "<br/><br/>\n",
    "\n",
    "**Feel free to use each function in the *bn_utils* file whenever you need**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_sample(query, evidence, cpts, graph, n):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampling\n",
    "Gibbs sampling is one of the Monte Carlo sampling methods which work based on random assignment.\n",
    "<br/>\n",
    "This algorithm is composed from 3 steps; 1.Fix evidence - 2.Initialize other variables - 3.Resampling\n",
    "<br/><br/>\n",
    "1. First, sort the vertices topologically (Done for you! you just need call the *topological_sort* function)\n",
    "<br/><br/>\n",
    "2. Fix the value of evidences\n",
    "    - iterate over all vertices\n",
    "    - if the vertex is evidence assign it by previous value\n",
    "<br/><br/>\n",
    "3. Sample each vertex in topological order\n",
    "    - if the vertex doesn't appear in the evidence, sample this vertex (you can use the *sample_vertex* method for sampling)\n",
    "<br/><br/>\n",
    "4. Take enough samples from the whole bayes net, say 10000\n",
    "<br/><br/>\n",
    "5. Calculate the approximate probability of the query respecting to the evidence.\n",
    "    - Calculate # of samples which are consistent with query\n",
    "    - The conditional probability is obtained by dividing the above value by the all samples\n",
    "<br/><br/>\n",
    "\n",
    "**Feel free to use each function in the *bn_utils* file whenever you need**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sample(query, evidence, cpts, graph, n):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Queries\n",
    "You should read the queries from the *path* and output:\n",
    "1. queries [list of dictionary]\n",
    "    - the *keys* are the query vertexes and the *values* are the value(*True/False*) of vertexes\n",
    "    - e.g: {V<sub>1</sub>:True, V<sub>2</sub>: True, ..., V<sub>m</sub>: False}\n",
    "<br/><br/>\n",
    "2. evidences [list of dictionary]\n",
    "    - the *keys* are the evidence vertexes and the *values* are the value(*True/False*) of vertexes\n",
    "    - e.g: {V<sub>1</sub>:True, V<sub>2</sub>: True, ..., V<sub>m</sub>: False}\n",
    "<br/><br/>\n",
    ">Note that the evidence at index ***i*** in *evidences* corresponds the the query at index ***i*** in *queries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpts, graph, V = load_model(IN_PATH)\n",
    "queries, evidences = read_queries(IN_PATH)\n",
    "#################### These are examples and should be omitted for final version ################\n",
    "# cpts = [[{0: True, 'Prob': 0.3}, {0: False, 'Prob': 0.7}], \n",
    "#         [{1: True, 'Prob': 0.6, 0: False}, {1: False, 'Prob': 0.4, 0: False}, {1: True, 'Prob': 0.1, 0: True}, {1: False, 'Prob': 0.9, 0: True}], \n",
    "#         [{2: True, 'Prob': 0.6, 0: False, 1: False}, {2: False, 'Prob': 0.4, 0: False, 1: False}, {2: True, 'Prob': 0.45, 0: False, 1: True},\n",
    "#          {2: False, 'Prob': 0.55, 0: False, 1: True}, {2: True, 'Prob': 0.5, 0: True, 1: False}, {2: False, 'Prob': 0.5, 0: True, 1: False},\n",
    "#          {2: True, 'Prob': 0.05, 0: True, 1: True}, {2: False, 'Prob': 0.95, 0: True, 1: True}], \n",
    "#         [{3: True, 'Prob': 0.35}, {3: False, 'Prob': 0.65}], \n",
    "#         [{4: True, 'Prob': 0.31, 3: False, 2: False}, {4: False, 'Prob': 0.69, 3: False, 2: False}, {4: True, 'Prob': 0.5, 3: False, 2: True},\n",
    "#          {4: False, 'Prob': 0.5, 3: False, 2: True}, {4: True, 'Prob': 0.75, 3: True, 2: False}, {4: False, 'Prob': 0.25, 3: True, 2: False},\n",
    "#          {4: True, 'Prob': 0.01, 3: True, 2: True}, {4: False, 'Prob': 0.99, 3: True, 2: True}]]\n",
    "\n",
    "# graph = {\n",
    "#     'parents_nodes':[[], [0], [0, 1], [], [3, 2]],\n",
    "#     'children_nodes': [[1, 2], [2], [4], [4], []]\n",
    "# }\n",
    "\n",
    "# V = 5\n",
    "# queries = [{0: True, 1: True}]\n",
    "# evidences = [{2: True, 4: True}]\n",
    "################## end of example ###################\n",
    "\n",
    "\n",
    "prior_ae_vals = []\n",
    "rejection_ae_vals = []\n",
    "likelihood_ae_vals = []\n",
    "gibbs_ae_vals = []\n",
    "for i in range(len(queries)):\n",
    "    exact_val = exact_inference(queries[i], evidences[i], cpts, graph, V)\n",
    "    prior = prior_sample(queries[i], evidences[i], cpts, graph, V)\n",
    "    rejection = rejection_sample(queries[i], evidences[i], cpts, graph, V)\n",
    "    likelihood = likelihood_sample(queries[i], evidences[i], cpts, graph, V)\n",
    "    gibbs = gibbs_sample(queries[i], evidences[i], cpts, graph)\n",
    "    ############ Start your code #################\n",
    "    # Add the desirable AE(Average Error) to the corresponding list for each method\n",
    "    # Replace 'None' with the correct statement\n",
    "    # None\n",
    "    # None\n",
    "    # None\n",
    "    # None\n",
    "\n",
    "# Illustrate the AE of each method with the draw_plot function\n",
    "# None\n",
    "\n",
    "########### End of your code ############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
