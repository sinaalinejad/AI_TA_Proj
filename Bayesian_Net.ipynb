{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%run \"bn_utils.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_PATH = \".\\inputs\"\n",
    "OUT_PATH = \".\\outputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_inference(query, evidence, cpts, graph):\n",
    "    new_cpts = []\n",
    "    parents = graph['parents_nodes']\n",
    "    for i, cpt in enumerate(cpts):\n",
    "        tb = []\n",
    "        for row in cpt:\n",
    "            if evidence.get(i) and evidence[i] != row[i]:\n",
    "                continue\n",
    "            flag = True\n",
    "            for j in parents[i]:\n",
    "                if evidence.get(j) and row[j] != evidence[j]:\n",
    "                    flag = False\n",
    "            if flag:\n",
    "                tb.append(row)\n",
    "        new_cpts.append(tb)\n",
    "    return variable_elimination(evidence, query, new_cpts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model\n",
    "You should read the bayesian network from the *path* and output:\n",
    "1. conditional probability tables (CPTs) [list of list of dictionary]\n",
    "    - each member (cpt) is a list (the list at index *i* is cpt of vertex *i*)\n",
    "    - each member of cpt is a dictionary:\n",
    "        - the dictionary contains *|parents(v)| + 2* keys. (the value of parents of *v*, the value of *v* and *'prob'*)\n",
    "        - e.g:\n",
    "            *{\n",
    "                V<sub>1</sub>: True,\n",
    "                V<sub>2</sub>: False,\n",
    "                ...\n",
    "                V<sub>v</sub>: True,\n",
    "                'prob': 0.66\n",
    "            }*\n",
    "<br/><br/>\n",
    "2. graph (bayesian network) [dictionary of list of list]\n",
    "    - the keys are *'parents_nodes'* and *'children_nodes'*\n",
    "    - the value of each key is a list of list; the element at index *i* is the parents/children of vertex *i*\n",
    "<br/><br/>\n",
    "3. V (the number variables/vertexes) [integer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior Sampling\n",
    "\n",
    "You should implement Prior Sampling.\n",
    "\n",
    "1. First, sort the vertices topologically (We have done this for you)\n",
    "\n",
    "2. Sample each vertex in topological order\n",
    "    - You can use np.random.random() function to generate a random number between 0 and 1\n",
    "<br/><br/>\n",
    "\n",
    "3. Take enough samples from the whole bayes net, say 10000\n",
    "\n",
    "4. Calculate the approximate probability of the query respecting to the evidence.\n",
    "    - Calculate # of samples that are consistent with the evidence\n",
    "    - Calculate # of samples that the query and the evidence occurred at the same time\n",
    "    - The conditional probability is obtained by dividing the first item by the second\n",
    "\n",
    ">Notice how this wouldn't work when we don't have samples that are consistent with evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_sample(query, evidence, cpts, graph, n):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rejection Sampling\n",
    "\n",
    "This is almost identical to Prior Sampling except that we reject samples that are inconsistent with the evidence.\n",
    "\n",
    "1. First, sort the vertices topologically (Done for you!)\n",
    "\n",
    "2. Sample each vertex in topological order\n",
    "    - You can use np.random.random() function to generate a random number between 0 and 1\n",
    "    - Do not continue sampling the whole bayes net if you encounter a sampled vertex which is not consistent with the evidence; This can be more efficient in the context of time and resource.\n",
    "<br/><br/>\n",
    "3. Take enough samples from the whole bayes net, say 10000\n",
    "\n",
    "4. Calculate the approximate probability of the query respecting to the evidence.\n",
    "    - Calculate # of samples that the query and the evidence occurred at the same time\n",
    "    - The conditional probability is obtained by dividing the above value by the total unrejected samples (notice that here all samples are consistent with the evidence because inconsistent sample were rejected)\n",
    "\n",
    ">Notice how this still can be resource and time consuming when you go down into a deep bayesian net and reject a whole sample just because of one inconsistent vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejection_sample(query, evidence, cpts, graph, n):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood Sampling\n",
    "\n",
    "To overcome the problem of rejected samples, we can force the samples to be consistent with the evidence without even sampling the vertices that appear in the evidence.\n",
    "\n",
    "1. First, sort the vertices topologically (Done for you!)\n",
    "\n",
    "2. Sample each vertex in topological order\n",
    "    - If the vertex appears in the evidence, just append the probability that the variable has the same value as in the evidence, to a list of weights\n",
    "    - Otherwise, sample the vertex as usual\n",
    "    - On sampling the whole bayes net, you should calculate the weight of that sample by multiplying the weights in the list of weights\n",
    "    - Add the sample's weight to a list, say sample_weights\n",
    "<br/><br/>\n",
    "\n",
    "3. Take enough samples from the whole bayes net, say 10000\n",
    "\n",
    "4. Calculate the approximate probability of the query respecting to the evidence.\n",
    "    - Calculate sum of samples' weights that the query and the evidence occurred at the same time\n",
    "    - The conditional probability is obtained by dividing the above value by the sum of all samples' weights\n",
    "\n",
    ">Notice that in this approach, when sampling vertices, we do not consider the evidence variables that are deeper in the bayes net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_sample(query, evidence, cpts, graph, n):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sample(query, evidence, cpts, graph, n):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Queries\n",
    "You should read the queries and from the *path* and output:\n",
    "1. queries [list of dictionary]\n",
    "    - the *keys* are the query vertexes and the *values* are the value(*True/False*) of vertexes\n",
    "    - e.g: {V<sub>1</sub>:True, V<sub>2</sub>: True, ..., V<sub>m</sub>: False}\n",
    "<br/><br/>\n",
    "2. evidences [list of dictionary]\n",
    "    - the *keys* are the evidence vertexes and the *values* are the value(*True/False*) of vertexes\n",
    "    - e.g: {V<sub>1</sub>:True, V<sub>2</sub>: True, ..., V<sub>m</sub>: False}\n",
    "<br/><br/>\n",
    ">Note that the evidence at index ***i*** in *evidences* corresponds the the query at index ***i*** in *queries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpts, graph, V = load_model(IN_PATH)\n",
    "queries, evidences = read_queries(IN_PATH)\n",
    "#################### These are examples and should be omitted for final version ################\n",
    "# cpts = [[{0: True, 'Prob': 0.3}, {0: False, 'Prob': 0.7}], \n",
    "#         [{1: True, 'Prob': 0.6, 0: False}, {1: False, 'Prob': 0.4, 0: False}, {1: True, 'Prob': 0.1, 0: True}, {1: False, 'Prob': 0.9, 0: True}], \n",
    "#         [{2: True, 'Prob': 0.6, 0: False, 1: False}, {2: False, 'Prob': 0.4, 0: False, 1: False}, {2: True, 'Prob': 0.45, 0: False, 1: True},\n",
    "#          {2: False, 'Prob': 0.55, 0: False, 1: True}, {2: True, 'Prob': 0.5, 0: True, 1: False}, {2: False, 'Prob': 0.5, 0: True, 1: False},\n",
    "#          {2: True, 'Prob': 0.05, 0: True, 1: True}, {2: False, 'Prob': 0.95, 0: True, 1: True}], \n",
    "#         [{3: True, 'Prob': 0.35}, {3: False, 'Prob': 0.65}], \n",
    "#         [{4: True, 'Prob': 0.31, 3: False, 2: False}, {4: False, 'Prob': 0.69, 3: False, 2: False}, {4: True, 'Prob': 0.5, 3: False, 2: True},\n",
    "#          {4: False, 'Prob': 0.5, 3: False, 2: True}, {4: True, 'Prob': 0.75, 3: True, 2: False}, {4: False, 'Prob': 0.25, 3: True, 2: False},\n",
    "#          {4: True, 'Prob': 0.01, 3: True, 2: True}, {4: False, 'Prob': 0.99, 3: True, 2: True}]]\n",
    "\n",
    "# graph = {\n",
    "#     'parents_nodes':[[], [0], [0, 1], [], [3, 2]],\n",
    "#     'children_nodes': [[1, 2], [2], [4], [4], []]\n",
    "# }\n",
    "\n",
    "# V = 5\n",
    "# queries = [{0: True, 1: True}]\n",
    "# evidences = [{2: True, 4: True}]\n",
    "################## end of example ###################\n",
    "\n",
    "\n",
    "prior_ae_vals = []\n",
    "rejection_ae_vals = []\n",
    "likelihood_ae_vals = []\n",
    "gibbs_ae_vals = []\n",
    "for i in range(len(queries)):\n",
    "    exact_val = exact_inference(queries[i], evidences[i], cpts, graph, V)\n",
    "    prior = prior_sample(queries[i], evidences[i], cpts, graph, V)\n",
    "    rejection = rejection_sample(queries[i], evidences[i], cpts, graph, V)\n",
    "    likelihood = likelihood_sample(queries[i], evidences[i], cpts, graph, V)\n",
    "    gibbs = gibbs_sample(queries[i], evidences[i], cpts, graph)\n",
    "    prior_ae_vals.append(abs(prior - exact_val))\n",
    "    rejection_ae_vals.append(abs(rejection - exact_val))\n",
    "    likelihood_ae_vals.append(abs(likelihood - exact_val))\n",
    "    gibbs_ae_vals.append(abs(gibbs - exact_val))\n",
    "\n",
    "\n",
    "draw_plot(prior_ae_vals, rejection_ae_vals, likelihood_ae_vals, gibbs_ae_vals, \"AE Graph\")\n",
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
